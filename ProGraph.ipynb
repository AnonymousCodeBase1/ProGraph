{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_num = 20\n",
    "class data(object):\n",
    "    def __init__(self):\n",
    "        self.IP = []\n",
    "        self.cert = []\n",
    "        self.urls = []\n",
    "        self.static_features = []\n",
    "        self.seq = []\n",
    "        self.label = []\n",
    "        self.time = []\n",
    "        self.image = []\n",
    "        self.fn = []\n",
    "        \n",
    "    def insert(self,IP,cert,urls,static_features,seq,label,time,image,fn):\n",
    "        self.IP.append(IP)\n",
    "        self.cert.append(cert)\n",
    "        self.urls.append(urls)\n",
    "        self.static_features.append(static_features)\n",
    "        self.seq.append(seq)\n",
    "        self.label.append(label)\n",
    "        self.time.append(time)\n",
    "        self.image.append(image)\n",
    "        self.fn.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interval import Interval\n",
    "import os,shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "class cert_cluster(object):\n",
    "    def __init__(self):\n",
    "        self.IPs = []\n",
    "        self.urls = {} #{urls:number}\n",
    "        self.url_number = [] # sorted (url:number)\n",
    "        self.url_weight = {}#{url:weight}\n",
    "        self.url_unique = []\n",
    "        self.certs = [] \n",
    "        self.sessions = []\n",
    "        self.images = []\n",
    "        self.static_features = []\n",
    "        self.fn = []\n",
    "        \n",
    "\n",
    "        self.seq = []\n",
    "        self.label = -1\n",
    "        self.tag = -1\n",
    "        self.time_slides = [] #(start,end)\n",
    "        \n",
    "        self.sim_list = []\n",
    "        self.url_all = []\n",
    "        \n",
    "        self.ip_weight = {}\n",
    "        self.cert_weight = {}\n",
    "        \n",
    "    #file name\n",
    "    def fn_cal(self):\n",
    "        self.fn = [ss['fn'] for ss in self.sessions]\n",
    "            \n",
    "        \n",
    "    #all urls\n",
    "    def all_url_cal(self):\n",
    "        for session in self.sessions:\n",
    "            urls = session['urls']\n",
    "            for url in urls:\n",
    "                if url not in self.url_all:\n",
    "                    self.url_all.append(url)\n",
    "    #url sim cal\n",
    "    def sim_cal(self,urls_dict):\n",
    "        self.sim_list = []\n",
    "        for url_dict_per in urls_dict:\n",
    "            score = 0\n",
    "            for url in self.url_unique:\n",
    "                if url in url_dict_per:\n",
    "                    score += 1\n",
    "            self.sim_list.append(score / (len(self.url_unique)*1.0))\n",
    "        if np.array(self.sim_list).max() > 0:\n",
    "            self.sim_list = self.sim_list/np.array(self.sim_list).max()\n",
    "        max_idx = np.argmax(np.array(self.sim_list))\n",
    "        for i in range(len(self.sim_list)):\n",
    "            if i!=max_idx:\n",
    "                self.sim_list[i] = 0\n",
    "            \n",
    "    #ip sim cal\n",
    "    def sim_ip_cal(self,urls_dict):\n",
    "        self.sim_list = []\n",
    "        for url_dict_per in urls_dict:\n",
    "            score = 0\n",
    "            for url in self.IPs:\n",
    "                if url in url_dict_per:\n",
    "                    score += 1\n",
    "            self.sim_list.append(score)\n",
    "        if np.array(self.sim_list).max() > 0:\n",
    "            self.sim_list = self.sim_list/np.array(self.sim_list).max()\n",
    "        max_idx = np.argmax(np.array(self.sim_list))\n",
    "        for i in range(len(self.sim_list)):\n",
    "            if i!=max_idx:\n",
    "                self.sim_list[i] = 0\n",
    "        \n",
    "            \n",
    "    \n",
    "    #static\n",
    "    def static_cal(self):\n",
    "        self.static_features = [a['static_features'] for a in self.sessions]\n",
    "    \n",
    "    \n",
    "    #major voting for cluster's label\n",
    "    def label_cal(self):\n",
    "        labels = [a['label'] for a in self.sessions]\n",
    "        self.label = max(labels,key = labels.count)\n",
    "        \n",
    "    def tag_cal(self):\n",
    "        tags = [a['tag'] for a in self.sessions]\n",
    "        self.tag = max(tags,key = tags.count)\n",
    "    \n",
    "    #image extract\n",
    "    def image_cal(self):\n",
    "        self.images = [item['image'].flatten() for item in self.sessions]\n",
    "    \n",
    "    #seq_ipdaate\n",
    "    def seq_cal(self):\n",
    "#         self.seq = [item['seq'].flatten() for item in self.sessions]\n",
    "        for i in range(len(self.sessions)):\n",
    "            session = self.sessions[i]\n",
    "            mat = session['seq']\n",
    "            sta = session['static_features']\n",
    "            for item in sta:\n",
    "                mat = np.append(mat,float(item))\n",
    "\n",
    "            self.seq.append(mat)\n",
    "            \n",
    "            \n",
    "        \n",
    "    #time_slieds generation\n",
    "    def time_cal(self):\n",
    "        def getfirst(item):\n",
    "            return item[0]\n",
    "        for session in self.sessions:\n",
    "            self.time_slides.append(session['time'])\n",
    "#         print(self.time_slides)\n",
    "        self.time_slides = sorted(self.time_slides,key=getfirst)\n",
    "        \n",
    "        #concat time period\n",
    "        self.time_slides = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in self.time_slides]\n",
    "#         print(len(self.time_slides))\n",
    "        while True:\n",
    "            big_flag = 0\n",
    "            for i in range(len(self.time_slides)):\n",
    "                \n",
    "                flag = 0\n",
    "                for j in range(len(self.time_slides)):\n",
    "#                     print((i,j))\n",
    "                    if i ==len(self.time_slides)-1 and j == len(self.time_slides)-1:\n",
    "                        big_flag =1\n",
    "                    if i==j:\n",
    "                        continue      \n",
    "                    if self.time_slides[i].overlaps(self.time_slides[j]):\n",
    "                        interval_merge = self.time_slides[i].join(self.time_slides[j])\n",
    "                        self.time_slides[i] = interval_merge\n",
    "                        del self.time_slides[j]\n",
    "\n",
    "                        flag = 1\n",
    "                        break                \n",
    "                if flag == 1:\n",
    "                    break\n",
    "            if big_flag==1:\n",
    "                break\n",
    "        self.time_slides = [(item.lower_bound,item.upper_bound) for item in self.time_slides]\n",
    "#         print(self.time_slides)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "                \n",
    "\n",
    "            \n",
    "        \n",
    "      \n",
    "    #新归纳session的url汇聚并排序计算\n",
    "    def urls_cal(self):\n",
    "        for item in self.sessions:\n",
    "            urls = item['urls']\n",
    "            for url in urls:\n",
    "                if url not in list(self.urls.keys()):\n",
    "                    self.urls[url] = 1\n",
    "                else:\n",
    "                    self.urls[url] += 1\n",
    "        #sorted\n",
    "        self.url_number = sorted(self.urls.items(),key=lambda item:item[1],reverse=True)\n",
    "    \n",
    "    #计算url重要系数\n",
    "    def urls_weight(self):\n",
    "        total = np.array([item[1] for item in self.url_number]).sum()*1.0\n",
    "        for item in self.url_number:\n",
    "            self.url_weight[item[0]] = item[1]*1.0/total\n",
    "                \n",
    "#         self.url_weight = [(item[0],item[1]*1.0/total) for item in self.url_number]\n",
    "        self.url_unique = [item[0] for item in self.url_number]\n",
    "        \n",
    "    #IP归纳\n",
    "    def ips_cal(self):\n",
    "        for item in self.sessions:\n",
    "            if item['IP'] not in self.IPs:\n",
    "                self.IPs.append(item['IP'])\n",
    "    \n",
    "    #cert归纳\n",
    "    def cert_cal(self):\n",
    "        for item in self.sessions:\n",
    "            if item['cert'] not in self.certs:\n",
    "                self.certs.append(item['cert'])\n",
    "        self.certs = [item for item in self.certs if item is not 0]\n",
    "    \n",
    "    #抛弃低频urls\n",
    "    def url_clean(self):\n",
    "        if len(self.url_number) <=5:\n",
    "            return\n",
    "        self.url_number = self.url_number[:5]\n",
    "        self.url_unique = [item[0] for item in self.url_number]\n",
    "        self.url_weight = {}\n",
    "        self.urls_weight()\n",
    "        \n",
    "    #ip_weigth\n",
    "    def ip_weight_cal(self):\n",
    "        self.ip_weight = {}\n",
    "        for session in self.sessions:\n",
    "            if session['IP'] not in list(self.ip_weight.keys()):\n",
    "                self.ip_weight[session['IP']] = 1.0/(len(self.sessions)*1.0)\n",
    "            else:\n",
    "                self.ip_weight[session['IP']] += 1.0/(len(self.sessions)*1.0)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    #cert_weigth\n",
    "    def cert_weight_cal(self):\n",
    "        self.cert_weight = {}\n",
    "        count = 0\n",
    "        for session in self.sessions:\n",
    "            if session['cert'] != 0 and session['cert'] not in list(self.cert_weight.keys()):\n",
    "                self.cert_weight[session['cert']] = 1\n",
    "                count +=1\n",
    "            elif session['cert'] != 0 and session['cert'] in list(self.cert_weight.keys()):\n",
    "                self.cert_weight[session['cert']] += 1\n",
    "                count +=1\n",
    "        if count != 0:\n",
    "            for key in list(self.cert_weight.keys()):\n",
    "                self.cert_weight[key] = self.cert_weight[key]/(count*1.0)\n",
    "    \n",
    "    \n",
    "    #更新cert_clt\n",
    "    def update(self):\n",
    "        self.urls_cal()\n",
    "        self.urls_weight()\n",
    "        self.ips_cal()\n",
    "        self.cert_cal()\n",
    "        self.seq_cal()\n",
    "        self.image_cal()\n",
    "        self.label_cal()\n",
    "        self.tag_cal()\n",
    "        self.static_cal()\n",
    "        self.url_clean()\n",
    "        self.all_url_cal()\n",
    "        self.fn_cal()\n",
    "        \n",
    "        self.cert_weight_cal()\n",
    "        self.ip_weight_cal()\n",
    "        \n",
    "#         self.time_cal()\n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def adj_matrix_gen(clusters):\n",
    "    def cert_sim(clt1,clt2):\n",
    "        if len(list(clt1.cert_weight.keys())) == 0 or len(list(clt2.cert_weight.keys())) == 0:\n",
    "            return 0\n",
    "        cross_keys = list(set(list(clt1.cert_weight.keys()))&set(list(clt2.cert_weight.keys())))\n",
    "        score = 0.0\n",
    "        for key in cross_keys:\n",
    "            score += clt1.cert_weight[key]*clt2.cert_weight[key]\n",
    "        return score\n",
    "    def ip_sim(clt1,clt2):\n",
    "        if len(list(clt1.ip_weight.keys())) == 0 or len(list(clt2.ip_weight.keys())) == 0:\n",
    "            return 0\n",
    "        cross_keys = list(set(list(clt1.ip_weight.keys()))&set(list(clt2.ip_weight.keys())))\n",
    "        score = 0.0\n",
    "        for key in cross_keys:\n",
    "            score += clt1.ip_weight[key]*clt2.ip_weight[key]\n",
    "        return score\n",
    "    \n",
    "    def url_sim(clt1,clt2):\n",
    "        if len(clt1.url_unique) == 0 or len(clt2.url_unique) == 0:\n",
    "            return 0\n",
    "        \n",
    "        overlaps = list(set(clt1.url_unique)&set(clt2.url_unique))\n",
    "        if len(overlaps) is 0:\n",
    "            return 0\n",
    "        res =  np.array([clt1.url_weight[key]*clt2.url_weight[key] for key in overlaps]).sum() / len(overlaps)\n",
    "#         print(res)\n",
    "        if res <0.3:\n",
    "            res = 0\n",
    "        return res\n",
    "#     def cert_sim(clt1,clt2):\n",
    "#         if len(clt1.certs) == 0 or len(clt2.certs) == 0:\n",
    "#             return 0\n",
    "#         return len(list(set(clt1.certs)&set(clt2.certs)))\n",
    "    \n",
    "#     def url_sim(clt1,clt2):\n",
    "#         if len(clt1.url_unique) == 0 or len(clt2.url_unique) == 0:\n",
    "#             return 0\n",
    "        \n",
    "#         overlaps = list(set(clt1.url_unique)&set(clt2.url_unique))\n",
    "#         if len(overlaps) is 0:\n",
    "#             return 0\n",
    "#         res =  np.array([clt1.url_weight[key]*clt2.url_weight[key] for key in overlaps]).sum() / len(overlaps)\n",
    "# #         print(res)\n",
    "#         if res <0.3:\n",
    "#             res = 0\n",
    "#         return res\n",
    "    \n",
    "    def time_sim(clt1,clt2):\n",
    "        clt1_time = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in clt1.time_slides]\n",
    "        clt2_time = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in clt2.time_slides]\n",
    "        count = 0\n",
    "        for clt1_t in clt1_time:\n",
    "            for clt2_t in clt2_time:\n",
    "                if clt1_t.overlaps(clt2_t):\n",
    "                    count+=1\n",
    "        if count <=7:\n",
    "            count = 0\n",
    "        return count\n",
    "    \n",
    "    mat = np.zeros((len(clusters),len(clusters)))\n",
    "#     print(111111111111)\n",
    "#     print(mat.shape)\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(i,len(clusters)):\n",
    "            if i==j:\n",
    "                continue\n",
    "            mat[i][j] = url_sim(clusters[i],clusters[j]) + cert_sim(clusters[i],clusters[j]) + ip_sim(clusters[i],clusters[j]) #+ time_sim(clusters[i],clusters[j])\n",
    "#             if math.isnan(url_sim(clusters[i],clusters[j])):\n",
    "#                 print(i,j)\n",
    "# #                 clusters[i].cert_weight_cal()\n",
    "#                 print(clusters[i].url_weight)\n",
    "#                 print(clusters[j].url_weight)\n",
    "            if i!=j:\n",
    "                mat[j][i] = mat[i][j]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clt_analysis(clts, adj_matrix):\n",
    "    inter_per_list = []\n",
    "    max_indx_list = []\n",
    "    outer_score_list_all = []\n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        if clts[i].tag == 0:\n",
    "            continue\n",
    "        inter_score = 0.0\n",
    "        outer_score = 0.0\n",
    "#         inter_score_list = [0.0 for i in range(10)]\n",
    "        outer_score_list = [0.0 for i in range(class_num)]\n",
    "        \n",
    "        for j in range(adj_matrix.shape[0]):\n",
    "            if i==j or clts[j].tag == 0:\n",
    "                continue\n",
    "            if clts[i].label == clts[j].label:\n",
    "                inter_score += adj_matrix[i][j]\n",
    "                outer_score_list[int(clts[i].label)] += adj_matrix[i][j]\n",
    "            else:\n",
    "                outer_score_list[int(clts[j].label)] += adj_matrix[i][j]\n",
    "                outer_score += adj_matrix[i][j]\n",
    "        if (inter_score+outer_score) != 0:\n",
    "            inter_per_list.append(inter_score/(inter_score+outer_score))\n",
    "        else:\n",
    "            inter_per_list.append(-1)\n",
    "            \n",
    "#         outer_score_list[clts[i].label] = adj_matrix[i][i]\n",
    "        max_indx_list.append(np.argmax(np.array(outer_score_list)))\n",
    "        outer_score_list_all.append(outer_score_list)\n",
    "    outer_clt = []\n",
    "    for i in range(len(inter_per_list)):\n",
    "        if inter_per_list[i] <=0.5 and inter_per_list[i] >= 0:\n",
    "            outer_clt.append((i,clts[i]))\n",
    "    max_indx_list = [(outer_score_list_all[i][clts[i].label],outer_score_list_all[i], len(clts[i].sessions)) for i in range(len(max_indx_list)) if max_indx_list[i] != int(clts[i].label) and np.array(max_indx_list[i]).sum()>0]\n",
    "    return outer_clt,max_indx_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhc_iso(clts,labels,mode = 'Normal',novel_classes = []):\n",
    "    def time_sim(clt1,clt2):\n",
    "        clt1_time = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in clt1.time_slides]\n",
    "        clt2_time = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in clt2.time_slides]\n",
    "        count = 0\n",
    "        for clt1_t in clt1_time:\n",
    "            for clt2_t in clt2_time:\n",
    "                if clt1_t.overlaps(clt2_t):\n",
    "                    count+=1\n",
    "        return count\n",
    "    \n",
    "    #deal with the isolated clts only\n",
    "    if mode == 'Normal':\n",
    "        zero_idx = [i for i in range(len(labels)) if labels[i] == -1 and clts[i].label not in novel_classes]\n",
    "    else:\n",
    "        zero_idx = [i for i in range(len(labels)) if labels[i] == -1]\n",
    "    \n",
    "#     sum_v = np.sum(adj_matrix,axis=0).reshape((adj_matrix.shape[0],1)) - np.array([adj_matrix[i][i] for i in range(adj_matrix.shape[0])]).reshape((adj_matrix.shape[0],1))\n",
    "#     zero_idx = [i for i in range(sum_v.shape[0]) if int(sum_v[i]) == 0]\n",
    "#     adj_matrix_new = adj_matrix.copy()\n",
    "    for idx in zero_idx:\n",
    "        #for each clt, we calculate the time sim with other clts\n",
    "        if mode!='Normal':\n",
    "            score_list = [0.0 for x in range(class_num)]\n",
    "        else:\n",
    "            score_list = [0.0 for x in range(class_num-novel_num)]\n",
    "        for j in range(len(clts)):\n",
    "            if j == idx:\n",
    "                continue\n",
    "            score = time_sim(clts[idx],clts[j])\n",
    "            score_list[labels[j]] += score\n",
    "        pred_label = np.argmax(np.array(score_list))\n",
    "        labels[idx] = pred_label\n",
    "        \n",
    "    return labels\n",
    "        \n",
    "def check_zero_label(adj_matrix,labels):\n",
    "    idxs = []\n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        if labels[i] == -1:\n",
    "            continue\n",
    "        flag = 1\n",
    "        count = 0\n",
    "        for j in range(adj_matrix.shape[0]):\n",
    "            if i==j:\n",
    "                continue\n",
    "            if labels[j] == -1:\n",
    "                continue\n",
    "            if labels[i] == labels[j] and adj_matrix[i][j] != 0:\n",
    "                flag = 0\n",
    "            elif labels[i] != labels[j] and adj_matrix[i][j] != 0:\n",
    "                count += adj_matrix[i][j]\n",
    "        if flag and count != 0:\n",
    "            idxs.append(i)\n",
    "    return idxs\n",
    "            \n",
    "def zero_adj(zero_idxs,clts, adj_matrix):\n",
    "    for idx in zero_idxs:\n",
    "        if clts[idx].tag == 0:\n",
    "            continue\n",
    "        cand = clts[idx]\n",
    "        score_list = [0.0 for i in range(class_num)]\n",
    "        for i in range(len(clts)):\n",
    "            if i==idx:\n",
    "                continue\n",
    "            score_list[clts[i].label] += adj_matrix[idx][i]\n",
    "        prob = np.array(softmax(score_list))\n",
    "        if np.argmax(prob) != clts[idx].label and prob[np.argmax(prob)] >= 0.9:\n",
    "            clts[idx].label = np.argmax(prob)\n",
    "    return clts\n",
    "        \n",
    "\n",
    "def check_zero(adj_matrix):\n",
    "    sum_v = np.sum(adj_matrix,axis=0).reshape((adj_matrix.shape[0],1)) - np.array([adj_matrix[i][i] for i in range(adj_matrix.shape[0])]).reshape((adj_matrix.shape[0],1))\n",
    "    zero_idx = [i for i in range(sum_v.shape[0]) if int(sum_v[i]) == 0]\n",
    "    print(len(zero_idx))\n",
    "    return zero_idx\n",
    "\n",
    "#adjust the label of outer clts\n",
    "def clt_adj(clts,o_clts):\n",
    "    def cert_sim(clt1,clt2):\n",
    "        if len(clt1.certs) == 0 or len(clt2.certs) == 0:\n",
    "            return 0\n",
    "        return len(list(set(clt1.certs)&set(clt2.certs)))\n",
    "    def ip_sim(clt1,clt2):\n",
    "        return len(list(set(clt1.IPs)&set(clt2.IPs)))\n",
    "    for o_idx,o_clt in o_clts:\n",
    "        clss = [0.0 for i in range(class_num)]\n",
    "        for i in range(len(clts)):\n",
    "            if i==o_idx:\n",
    "                continue\n",
    "            sim = cert_sim(o_clt,clts[i]) + ip_sim(o_clt,clts[i])\n",
    "            clss[clts[i].label] += sim\n",
    "        if np.array(clss).max() < 2:\n",
    "            print(\"{} no need for adjustment\".format(o_idx))\n",
    "            continue\n",
    "        if np.argmax(np.array(clss)) != o_clt.label:\n",
    "            print(\"{}: {} -> {}\".format(o_idx, o_clt.label,np.argmax(np.array(clss))))\n",
    "            clts[o_idx].label = np.argmax(np.array(clss))\n",
    "                           \n",
    "    return clts\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x - np.max(x))/(np.sum(np.exp(x - np.max(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def spread(clts,adj_matrix,labels,th=None,mode = 'Normal'):\n",
    "    #predict the unlabel clts\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] != -1:\n",
    "            continue\n",
    "        if mode !='Normal':\n",
    "            score_list = [0.0 for x in range(class_num)]\n",
    "        else:\n",
    "            score_list = [0.0 for x in range(class_num-novel_num)]\n",
    "            \n",
    "        for j in range(len(labels)):\n",
    "            if i==j: #ignore the self-node\n",
    "                continue\n",
    "            if labels[j] == -1: #ignore the unlabel testing nodes\n",
    "                continue\n",
    "            score_list[labels[j]] += adj_matrix[i][j]\n",
    "        if np.array(score_list).sum() == 0:\n",
    "            continue\n",
    "#         print(score_list)\n",
    "        score_list = np.array(softmax(score_list))\n",
    "        \n",
    "        \n",
    "#         print(score_list)\n",
    "        if mode =='Normal':\n",
    "            if score_list.max() < th:\n",
    "                continue\n",
    "#         print(\"maxvalue: {} maxlabel: {} gt: {}\".format(max(score_list),np.argmax(score_list),clts[i].label))\n",
    "        \n",
    "        \n",
    "        pred_label = np.argmax(score_list)\n",
    "        #tag the predicted label\n",
    "        labels[i] = pred_label\n",
    "#         print(pred_label)\n",
    "    return labels\n",
    "\n",
    "def acc_per_round(clts,labels,novel_idxs):\n",
    "#     gt_labels = [[clt.label]*len(clt.sessions) for clt in clts if clt.tag==0 and clt.label!=-1]\n",
    "    gt_labels = [[clts[i].label]*len(clts[i].sessions) for i in range(len(clts)) if clts[i].tag==0 and labels[i]!=-1 and i not in novel_idxs]\n",
    "    labels_pred = [[labels[i]]*len(clts[i].sessions) for i in range(len(clts)) if clts[i].tag==0 and labels[i]!=-1 and i not in novel_idxs]\n",
    "    gt_labels = [item0 for item in gt_labels for item0 in item]\n",
    "    labels_pred = [item0 for item in labels_pred for item0 in item]\n",
    "    acc = accuracy_score(gt_labels,labels_pred)\n",
    "    pre = recall_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "    rec = precision_score(gt_labels,labels_pred, average=\"weighted\")\n",
    "    f1 = f1_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "    return acc,pre,rec,f1\n",
    "    \n",
    "    acc = 0\n",
    "    al = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i]==-1 or clts[i].tag == 1:\n",
    "            continue\n",
    "        al += 1\n",
    "        if labels[i] == clts[i].label:\n",
    "            acc += 1\n",
    "    return acc*1.0/(al*1.0)\n",
    "\n",
    "def aggregation(clts,mat,labels):\n",
    "    #ip,cert,url weight aggragation\n",
    "    def agg_weight(clt_c,clt_n,weight):\n",
    "        #ip agg\n",
    "        for ip in list(clt_n.ip_weight.keys()):\n",
    "            if ip not in list(clt_c.ip_weight.keys()):\n",
    "                clt_c.ip_weight[ip] = clt_n.ip_weight[ip] * weight\n",
    "            else:\n",
    "                clt_c.ip_weight[ip] += clt_c.ip_weight[ip] * clt_n.ip_weight[ip] * weight\n",
    "        #cert agg\n",
    "        for cert in list(clt_n.cert_weight.keys()):\n",
    "            if cert not in list(clt_c.cert_weight.keys()):\n",
    "                clt_c.cert_weight[cert] = clt_n.cert_weight[cert] * weight\n",
    "            else:\n",
    "                clt_c.cert_weight[cert] += clt_c.cert_weight[cert] * clt_n.cert_weight[cert] * weight\n",
    "                \n",
    "        #url agg\n",
    "        for url in list(clt_n.url_weight.keys()):\n",
    "            if url not in list(clt_c.url_weight.keys()):\n",
    "                clt_c.url_weight[url] = clt_n.url_weight[url] * weight\n",
    "            else:\n",
    "                clt_c.url_weight[url] += clt_c.url_weight[url] * clt_n.url_weight[url] * weight\n",
    "                \n",
    "        \n",
    "    \n",
    "    #aggregate the labeled nodes\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == -1:\n",
    "            continue\n",
    "        #aggragate the labeled nodes from same class\n",
    "        softmax_mat = []\n",
    "        for k in range(len(labels)):\n",
    "            if labels[k]!=-1 and labels[k]==labels[i]:\n",
    "                softmax_mat.append(mat[i][k])\n",
    "            else:\n",
    "                softmax_mat.append(0) \n",
    "        for j in range((len(labels))):\n",
    "            if i==j or labels[i] != labels[j]:\n",
    "                continue\n",
    "            #we aggregate the labled nodes from same class(so as to calculate the softmax)\n",
    "            agg_weight(clts[i],clts[j],softmax(softmax_mat)[j]) \n",
    "    #update the edge with the unlabeled nodes\n",
    "    adj_matrix = adj_matrix_gen(clts)\n",
    "    return adj_matrix\n",
    "            \n",
    "def label_update(clts,labels):\n",
    "    labels = []\n",
    "    for clt in clts:\n",
    "        if clt.tag == 0:\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(clt.label)\n",
    "    labels = np.array(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_init(mode,reverse = False, novelty = 0, test_rate = 0,novel_classes = [],seed = 0):\n",
    "    #novel class isolation\n",
    "    if novelty:\n",
    "        np.random.seed(seed)\n",
    "        if len(novel_classes) == 0:\n",
    "            for i in range(novelty):\n",
    "                while True:\n",
    "                    can = np.random.randint(class_num)\n",
    "                    if can not in novel_classes:\n",
    "                        novel_classes.append(can)\n",
    "                        break\n",
    "    map_novel_to_known = {}\n",
    "    map_known_to_novel = {}\n",
    "    known_classes = [i for i in range(class_num-novelty,class_num)]\n",
    "    novel_for_map = list(set(novel_classes) -(set(novel_classes)&set(known_classes)))\n",
    "    known_for_map = list(set(known_classes) -(set(novel_classes)&set(known_classes)))\n",
    "    for i in range(len(known_for_map)):\n",
    "        map_novel_to_known[novel_for_map[i]] = known_for_map[i]\n",
    "        map_known_to_novel[known_for_map[i]] = novel_for_map[i]\n",
    "#     print(map_novel_to_known)\n",
    "#     print(map_known_to_novel)\n",
    "#     print(novel_for_map)\n",
    "#     print(known_for_map)\n",
    "    #load traning data from net_A\n",
    "    if reverse:\n",
    "        with open(\"poor.pkl\",'rb') as file:\n",
    "            dataT  = pickle.loads(file.read())\n",
    "    else:\n",
    "        with open(\"normal.pkl\",'rb') as file:\n",
    "            dataT  = pickle.loads(file.read())\n",
    "\n",
    "    data_list = []\n",
    "    data_list_novel = []\n",
    "\n",
    "    for i in range(len(dataT.label)):\n",
    "        tmp = {}\n",
    "        tmp['IP'] = dataT.static_feature[i][0]\n",
    "        if len(dataT.cert_number[i]) is 0:\n",
    "            tmp['cert'] = 0\n",
    "        else:\n",
    "            tmp['cert'] = dataT.cert_number[i][0]\n",
    "        tmp['urls'] = [item for item in dataT.urls[i] if item != '0']\n",
    "        tmp['static_features'] = np.array([float(item) for item in dataT.static_feature[i][1:-2]])\n",
    "        tmp['seq'] = dataT.seq_matirx[i]\n",
    "        if novelty:\n",
    "            if dataT.label[i] in novel_classes:\n",
    "                if dataT.label[i] in novel_for_map:\n",
    "                    tmp['label'] = map_novel_to_known[dataT.label[i]]\n",
    "                else:\n",
    "                    tmp['label'] = dataT.label[i]\n",
    "            else:\n",
    "                if dataT.label[i] in known_for_map:\n",
    "                    tmp['label'] = map_known_to_novel[dataT.label[i]]\n",
    "                else:\n",
    "                    tmp['label'] = dataT.label[i]\n",
    "        else:       \n",
    "            tmp['label'] = dataT.label[i]\n",
    "        tmp['time'] = (float(dataT.static_feature[i][-2]),float(dataT.static_feature[i][-1]))\n",
    "        tmp['image'] = dataT.image[i]\n",
    "        tmp['fn'] = dataT.fn[i]\n",
    "        tmp['tag'] = 1\n",
    "        if novelty:\n",
    "            if dataT.label[i] in novel_classes:\n",
    "                data_list_novel.append(tmp)\n",
    "            else:\n",
    "                data_list.append(tmp)\n",
    "        else:\n",
    "            data_list.append(tmp)\n",
    "    print(\"{} sessions loaded from training set.\".format(len(data_list)))\n",
    "    if novelty:\n",
    "        print(\"{} sessions are separated as novel set.\".format(len(data_list_novel)))\n",
    "\n",
    "    IP_clusters = []\n",
    "    used_sessions = []\n",
    "\n",
    "    #inital IP clusters\n",
    "    for i in range(len(data_list)):\n",
    "        session = data_list[i]\n",
    "        IP = session['IP']\n",
    "        if len(IP_clusters) == 0:\n",
    "            used_sessions.append(i)\n",
    "            clt = cert_cluster()\n",
    "            clt.sessions.append(session)\n",
    "            clt.update()\n",
    "            IP_clusters.append(clt)\n",
    "        else:\n",
    "            d = 0\n",
    "            for j in range(len(IP_clusters)):\n",
    "                if IP in IP_clusters[j].IPs:\n",
    "                    IP_clusters[j].sessions.append(session)\n",
    "                    IP_clusters[j].update()\n",
    "                    used_sessions.append(i)\n",
    "                    d = 1\n",
    "                    break\n",
    "\n",
    "            if d == 0:\n",
    "                used_sessions.append(i)\n",
    "                clt = cert_cluster()\n",
    "                clt.sessions.append(session)\n",
    "                clt.update()\n",
    "                IP_clusters.append(clt)\n",
    "\n",
    "    data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "    for item in IP_clusters:\n",
    "        item.time_cal()\n",
    "    if mode == 'cross':\n",
    "        print(\"{} clusters are initialized for training set.\".format(len(IP_clusters)))\n",
    "        \n",
    "    if mode == 'cross':\n",
    "        if reverse:\n",
    "            with open(\"normal.pkl\",'rb') as file:\n",
    "                dataT  = pickle.loads(file.read())\n",
    "        else:\n",
    "            with open(\"poor.pkl\",'rb') as file:\n",
    "                dataT  = pickle.loads(file.read())\n",
    "        data_list_novel_test = []\n",
    "\n",
    "        for i in range(len(dataT.label)):\n",
    "            tmp = {}\n",
    "            tmp['IP'] = dataT.static_feature[i][0]\n",
    "            if len(dataT.cert_number[i]) is 0:\n",
    "                tmp['cert'] = 0\n",
    "            else:\n",
    "                tmp['cert'] = dataT.cert_number[i][0]\n",
    "            tmp['urls'] = [item for item in dataT.urls[i] if item != '0']\n",
    "            tmp['static_features'] = np.array([float(item) for item in dataT.static_feature[i][1:-2]])\n",
    "            tmp['seq'] = dataT.seq_matirx[i]\n",
    "            if novelty:\n",
    "                if dataT.label[i] in novel_classes:\n",
    "                    if dataT.label[i] in novel_for_map:\n",
    "                        tmp['label'] = map_novel_to_known[dataT.label[i]]\n",
    "                    else:\n",
    "                        tmp['label'] = dataT.label[i]\n",
    "                else:\n",
    "                    if dataT.label[i] in known_for_map:\n",
    "                        tmp['label'] = map_known_to_novel[dataT.label[i]]\n",
    "                    else:\n",
    "                        tmp['label'] = dataT.label[i]\n",
    "            else:       \n",
    "                tmp['label'] = dataT.label[i]\n",
    "            tmp['time'] = (float(dataT.static_feature[i][-2]),float(dataT.static_feature[i][-1]))\n",
    "            tmp['image'] = dataT.image[i]\n",
    "            tmp['fn'] = dataT.fn[i]\n",
    "            tmp['tag'] = 0\n",
    "            if novelty:\n",
    "                if dataT.label[i] in novel_classes:\n",
    "                    data_list_novel_test.append(tmp)\n",
    "                    data_list.append(tmp)\n",
    "                else:\n",
    "                    data_list.append(tmp)\n",
    "            else:\n",
    "                data_list.append(tmp)\n",
    "        print(\"{} sessions loaded from testing set.\".format(len(data_list)))\n",
    "        if novelty:\n",
    "            print(\"{} sessions are separated as testing novel set.\".format(len(data_list_novel_test)))\n",
    "\n",
    "        IP_clusters_poor = []\n",
    "        used_sessions = []\n",
    "\n",
    "        #inital IP clusters\n",
    "        for i in range(len(data_list)):\n",
    "            session = data_list[i]\n",
    "            IP = session['IP']\n",
    "            if len(IP_clusters_poor) == 0:\n",
    "                used_sessions.append(i)\n",
    "                clt = cert_cluster()\n",
    "                clt.sessions.append(session)\n",
    "                clt.update()\n",
    "                IP_clusters_poor.append(clt)\n",
    "            else:\n",
    "                d = 0\n",
    "                for j in range(len(IP_clusters_poor)):\n",
    "                    if IP in IP_clusters_poor[j].IPs:\n",
    "                        IP_clusters_poor[j].sessions.append(session)\n",
    "                        IP_clusters_poor[j].update()\n",
    "                        used_sessions.append(i)\n",
    "                        d = 1\n",
    "                        break\n",
    "\n",
    "                if d == 0:\n",
    "                    used_sessions.append(i)\n",
    "                    clt = cert_cluster()\n",
    "                    clt.sessions.append(session)\n",
    "                    clt.update()\n",
    "                    IP_clusters_poor.append(clt)\n",
    "\n",
    "        data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "        for item in IP_clusters_poor:\n",
    "            item.time_cal()\n",
    "        IP_clusters.extend(IP_clusters_poor)\n",
    "        \n",
    "        print(\"{} clusters are initialized for testing set.\".format(len(IP_clusters_poor)))\n",
    "        print(\"{} nodes are included in the initialized graph.\".format(len(IP_clusters)))\n",
    "        \n",
    "        #initial testing set\n",
    "        \n",
    "        if novelty:\n",
    "            return IP_clusters,novel_classes\n",
    "        else:\n",
    "            return IP_clusters\n",
    "    else:\n",
    "        #construct testing set\n",
    "        label_idx_dict = {}\n",
    "        for i in range(len(IP_clusters)):\n",
    "            clt = IP_clusters[i]\n",
    "            if clt.label not in list(label_idx_dict.keys()):\n",
    "                label_idx_dict[clt.label] = [i]\n",
    "            else:\n",
    "                label_idx_dict[clt.label].append(i)\n",
    "        test_idx = []\n",
    "        for key in list(label_idx_dict.keys()):\n",
    "            idxs = np.arange(len(label_idx_dict[key]))\n",
    "#             print(idxs)\n",
    "            np.random.shuffle(idxs)\n",
    "            idxs = idxs[:int(idxs.shape[0]*test_rate)]\n",
    "            test_idx.extend(np.array(label_idx_dict[key])[list(idxs)])\n",
    "        for idx in test_idx:\n",
    "            IP_clusters[idx].tag = 0\n",
    "        print(\"{} clusters are initialized for training set.\".format(len(IP_clusters) - len(test_idx)))\n",
    "        print(\"{} clusters are initialized for testing set.\".format(len(test_idx)))\n",
    "        print(\"{} nodes are included in the initialized graph.\".format(len(IP_clusters)))\n",
    "        if novelty:\n",
    "            return IP_clusters,novel_classes\n",
    "        else:\n",
    "            return IP_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mat_init(IP_clusters):\n",
    "#     print([item.tag for item in IP_clusters])\n",
    "    adj_matrix = adj_matrix_gen(IP_clusters)\n",
    "#     print([item.tag for item in IP_clusters])\n",
    "    labels = []\n",
    "    for clt in IP_clusters:\n",
    "        if clt.tag == 0:\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(clt.label)\n",
    "    labels = np.array(labels)\n",
    "    return adj_matrix,labels\n",
    "\n",
    "def mark_init_novel_index(clts,novel_classes):\n",
    "    return [idx for idx in range(len(clts)) if clts[idx].tag==0 and clts[idx].label in novel_classes]\n",
    "\n",
    "def check_novel_label(clts,labels,novel_idxs):\n",
    "    total_correct = [clts[i] for i in range(len(labels)) if labels[i]==-1 and i in novel_idxs]\n",
    "    total_correct = np.array([len(clt.sessions) for clt in total_correct]).sum()\n",
    "    \n",
    "    total_sessions = [clts[i] for i in range(len(clts)) if i in novel_idxs]\n",
    "    total_sessions = np.array([len(clt.sessions) for clt in total_sessions]).sum()\n",
    "#     print(total_sessions)\n",
    "#     for idx in novel_idxs:\n",
    "#         print(\"gt: {} pred: {}\".format(clts[idx].label,labels[idx]))\n",
    "    print(\"Novelty detection rate: {}\".format((total_correct*1.0)/total_sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdns = ['digicert.com','w3.org','com.cn','.cc','usertrust.com','rapidssl.com',\n",
    "  'digicert.cn','mmstat.com','alicdn.com','adobe.com','x02.cc', 'xf6.cc', 'x1aB.cn',\n",
    "    'globalsign.com','digicert-cn.com','globalsign.net','digicert-an.com','digitalcertvalidation.com','scorecardresearch.com','cr-nielsen.com','gtimg.com']\n",
    "\n",
    "# cdns = []\n",
    "\n",
    "def acc_per_round_novel(clts,labels):\n",
    "#     gt_labels = [[clt.label]*len(clt.sessions) for clt in clts if clt.tag==0 and clt.label!=-1]\n",
    "    gt_labels = [[clts[i].label]*len(clts[i].sessions) for i in range(len(clts))]\n",
    "    labels_pred = [[labels[i]]*len(clts[i].sessions) for i in range(len(clts))]\n",
    "    gt_labels = [item0 for item in gt_labels for item0 in item]\n",
    "    labels_pred = [item0 for item in labels_pred for item0 in item]\n",
    "    acc = accuracy_score(gt_labels,labels_pred)\n",
    "    pre = recall_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "    rec = precision_score(gt_labels,labels_pred, average=\"weighted\")\n",
    "    f1 = f1_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "    return acc,pre,rec,f1\n",
    "\n",
    "def novel_clt_clf(clts,labels,novel_idxs,novel_classes=[15,16,17,18,19]):\n",
    "    def merge(list1,i,j):\n",
    "        list1[i].extend(list1[j])\n",
    "        del(list1[j])\n",
    "        return list1\n",
    "    def loop(list1):\n",
    "        for i in range(len(urls_list)):\n",
    "            if len(urls_list[i]) == 0:\n",
    "                continue\n",
    "            for j in range(len(urls_list)):\n",
    "                if len(urls_list[j]) == 0 or i==j:\n",
    "                    continue\n",
    "                if urls_list[i] == urls_list[j]:\n",
    "#                     print(urls_list[i])\n",
    "#                     print(urls_list[j])\n",
    "                    return i,j\n",
    "        return -1,-1\n",
    "    def getSecond(item):\n",
    "        return item[1]\n",
    "    clts_novel_idxs = [i for i in novel_idxs if labels[i] == -1]\n",
    "    clts_cert_idxs = [i for i in clts_novel_idxs if len(clts[i].certs) != 0]\n",
    "    clts_no_cert_idxs = [i for i in clts_novel_idxs if len(clts[i].certs) == 0]\n",
    "#     print([(clt.label,clt.url_number) for clt in clts_no_cert])\n",
    "    \n",
    "    hashtable = [] #[[certs]]\n",
    "    cert_urls = {} #[idx] to [urls]\n",
    "    for i in clts_cert_idxs:\n",
    "        key = None\n",
    "        certs =  list(np.unique(np.array(clts[i].certs)))\n",
    "#         print(certs)\n",
    "        for k in range(len(hashtable)):\n",
    "            if len(set(certs) & set(hashtable[k]))!= 0:\n",
    "                key = k\n",
    "        \n",
    "        \n",
    "        if key == None:\n",
    "            hashtable.append(certs)\n",
    "            cert_urls[hashtable.index(certs)] = []\n",
    "            key = hashtable.index(certs)\n",
    "#             print(certs)\n",
    "#             hashtable.append(certs)\n",
    "#         print(clts[i].url_number)\n",
    "        if len(clts[i].url_number) == 0:\n",
    "#             print(clts[i].url_number)\n",
    "#             print(certs)\n",
    "            continue\n",
    "#         print(hashtable)\n",
    "#         print(cert_urls)\n",
    "#         print(key)\n",
    "        url = clts[i].url_number[0][0]\n",
    "        if url in cdns:\n",
    "            continue\n",
    "#         if url not in list(cert_urls[key].keys()):\n",
    "#             cert_urls[key][url] = 0\n",
    "#         cert_urls[key][url] += 1\n",
    "        if clts[i].url_number[0][0] not in cert_urls[key]:\n",
    "            cert_urls[key].append(clts[i].url_number[0][0])\n",
    "#     print(cert_urls)\n",
    "    \n",
    "    #merge the certs with same urls\n",
    "    urls_list =  list(cert_urls.values())\n",
    "#     print(urls_list)\n",
    "    while True:\n",
    "        idx0,idx1 = loop(urls_list)\n",
    "#         print(idx0,idx1)\n",
    "        if idx0==-1:\n",
    "            break\n",
    "        urls_list = merge(urls_list,idx0,idx1)\n",
    "        hashtable = merge(hashtable,idx0,idx1)\n",
    "                    \n",
    "#     print(urls_list)\n",
    "#     print(hashtable)\n",
    "    \n",
    "    clusters = []\n",
    "#     used_idxs = []\n",
    "    urls_known_list = [cert_urls[key] for key in list(cert_urls.keys())]\n",
    "    for x in range(len(hashtable)):\n",
    "        clusters.append([])\n",
    "    urls_known_list = [item for item0 in urls_known_list for item in item0]\n",
    "        \n",
    "        \n",
    "    #label the clts with certs\n",
    "    for i in clts_cert_idxs:\n",
    "        for cert_can in hashtable:\n",
    "            if len(list(set(clts[i].certs)&set(cert_can))) != 0:\n",
    "                clusters[hashtable.index(cert_can)].append(i)\n",
    "#                 used_idxs.append(i)\n",
    "                break\n",
    "    #extract urls from the remaining clts\n",
    "    urls_no_cert = []\n",
    "    for i in clts_no_cert_idxs:\n",
    "        if len(clts[i].url_number) == 0:\n",
    "            continue\n",
    "        urls = [item[0] for item in clts[i].url_number if item[0] not in cdns]\n",
    "        for url in urls:\n",
    "            if url in urls_known_list:\n",
    "                break\n",
    "            if url not in [item[0] for item in urls_no_cert]:\n",
    "                urls_no_cert.append((url,0))\n",
    "            urls_no_cert[[item[0] for item in urls_no_cert].index(url)] = (url,urls_no_cert[[item[0] for item in urls_no_cert].index(url)][1]+1)\n",
    "#     print(urls_no_cert)\n",
    "    urls_no_cert = sorted(urls_no_cert,key=getSecond,reverse = True)\n",
    "#     print(urls_no_cert)\n",
    "    # label the remaining primary clts\n",
    "    for rm_classes_num in range(2): #(len(novel_classes) - len(clusters)):\n",
    "        matched_url = urls_no_cert[rm_classes_num][0]\n",
    "        clt_tmp = []\n",
    "        for i in clts_no_cert_idxs:\n",
    "            urls = [item[0] for item in clts[i].url_number if item[0] not in cdns]\n",
    "#             print(urls)\n",
    "            if matched_url in urls:\n",
    "                clt_tmp.append(i)\n",
    "        clusters.append(clt_tmp)\n",
    "            \n",
    "        \n",
    "    # label the clts with url and matched urls\n",
    "#     for i in clts_cert_idxs:\n",
    "#         if len(clts[i].url_number) != 0:\n",
    "#             urls = [item[0] for item in clts[i].url_number if item[0] not in cdns]\n",
    "#             score = []\n",
    "#             for urls_list in urls_known_list:\n",
    "#                 score.append(len(set(urls_list)&set(urls)))\n",
    "# #                 print((urls,urls_list))\n",
    "#             clusters[np.argmax(np.array(score))].append(i)\n",
    "#             print((urls_known_list[np.argmax(np.array(score))],urls))\n",
    "#             print(score)\n",
    "#     for item0 in clusters:\n",
    "#         print([clts[item1].label for item1 in item0])\n",
    "        \n",
    "        \n",
    "    #novel graph init\n",
    "    labels_novel = [] #new\n",
    "    clts_novel = []\n",
    "    novel_idx_mapping = {} #old to new\n",
    "    for i in novel_idxs:\n",
    "        if i not in [item1 for item in clusters for item1 in item]:\n",
    "            labels_novel.append(-1)\n",
    "        else:\n",
    "            labels_novel.append(clts[i].label)\n",
    "            \n",
    "        novel_idx_mapping[i] = len(labels_novel)-1\n",
    "#         clts[i].url_weight ={}\n",
    "        clts[i].update()\n",
    "#         clts[i].url_weight = [(item[0],item[1]*1.0/(np.array([item[1] for item in clts[i].url_number]).sum()*1.0)) for item in clts[i].url_number]\n",
    "#         print(clts[i].url_weight)\n",
    "        clts_novel.append(clts[i])\n",
    "    \n",
    "    \n",
    "    adj_novel = adj_matrix_gen(clts_novel)\n",
    "    \n",
    "#     print(adj_novel.shape)\n",
    "#     print(labels_novel)\n",
    "#     print(list(adj_novel.flatten()))\n",
    "    \n",
    "    #propagate the novel adj\n",
    "    for e in range(3):\n",
    "        # nodes aggregate info from neighbours and update the adjoin edges\n",
    "        adj_novel = aggregation(clts_novel,adj_novel,labels_novel)\n",
    "        # update the labels\n",
    "    #     labels = label_update(clts_adj,labels)\n",
    "        print(len([item for item in labels_novel if item==-1]))\n",
    "        # spread label info to the unlabeled neighbours\n",
    "        labels_novel = spread(clts_novel,adj_novel,labels_novel,mode=\"novel\")\n",
    "        # performing measurement in testing set\n",
    "        acc,pre,rec,f1 = acc_per_round_novel(clts_novel,labels_novel)\n",
    "        f1 = 2*pre*rec/(pre+rec)\n",
    "        print(\"Epoch : {}  Acc: {} pre: {} rec: {} f1: {}.\".format(e,acc,pre,rec,f1))\n",
    "    print(len([item for item in labels_novel if item==-1]))\n",
    "    \n",
    "    labels_novel = enhc_iso(clts_novel,labels_novel,mode=\"novel\")\n",
    "    acc,pre,rec,f1 = acc_per_round_novel(clts_novel,labels_novel)\n",
    "    f1 = 2*pre*rec/(pre+rec)\n",
    "    print(\"Novelty Detection :  Acc: {} pre: {} rec: {} f1: {}.\".format(acc,pre,rec,f1))\n",
    "    \n",
    "    #adjust the original labels\n",
    "    for i in novel_idxs:\n",
    "        labels[i] = labels_novel[novel_idx_mapping[i]]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855 sessions loaded from training set.\n",
      "896 sessions are separated as novel set.\n",
      "433 clusters are initialized for training set.\n",
      "4429 sessions loaded from testing set.\n",
      "1227 sessions are separated as testing novel set.\n",
      "918 clusters are initialized for testing set.\n",
      "1351 nodes are included in the initialized graph.\n",
      "[1, 2, 3, 4, 0]\n",
      "*** Confused nodes should be adjusted to the ground true labels.***\n",
      "45: 6 -> 3\n",
      "61 no need for adjustment\n",
      "100 no need for adjustment\n",
      "107 no need for adjustment\n",
      "110 no need for adjustment\n",
      "111 no need for adjustment\n",
      "137 no need for adjustment\n",
      "153 no need for adjustment\n",
      "154 no need for adjustment\n",
      "174 no need for adjustment\n",
      "175: 9 -> 11\n",
      "180 no need for adjustment\n",
      "184 no need for adjustment\n",
      "185 no need for adjustment\n",
      "198 no need for adjustment\n",
      "203: 10 -> 3\n",
      "207 no need for adjustment\n",
      "216: 11 -> 10\n",
      "217: 11 -> 7\n",
      "230 no need for adjustment\n",
      "234: 11 -> 4\n",
      "235 no need for adjustment\n",
      "242 no need for adjustment\n",
      "243 no need for adjustment\n",
      "307: 4 -> 2\n",
      "344 no need for adjustment\n",
      "407 no need for adjustment\n",
      "413 no need for adjustment\n",
      "416 no need for adjustment\n",
      "425 no need for adjustment\n",
      "430 no need for adjustment\n",
      "61 no need for adjustment\n",
      "100 no need for adjustment\n",
      "107 no need for adjustment\n",
      "111 no need for adjustment\n",
      "153 no need for adjustment\n",
      "198 no need for adjustment\n",
      "207 no need for adjustment\n",
      "235 no need for adjustment\n",
      "\n",
      "Starting to propagate.\n",
      "918\n",
      "Epoch : 0  Acc: 0.8469428007889547 pre: 0.8469428007889547 rec: 0.9030256874104157 f1: 0.8740855735827573.\n",
      "385\n",
      "Testing Acc: 0.831108351443651 pre: 0.831108351443651 rec: 0.8885899989372836 f1: 0.8588885009542414 after 1 epochs.\n",
      "\n",
      "**************** Novelty Detection ****************\n",
      "Novelty detection rate: 0.9536423841059603\n",
      "135\n",
      "Epoch : 0  Acc: 0.8940397350993378 pre: 0.8940397350993378 rec: 0.9981720377163397 f1: 0.9432405790981334.\n",
      "34\n",
      "Epoch : 1  Acc: 0.8940397350993378 pre: 0.8940397350993378 rec: 0.9981720377163397 f1: 0.9432405790981334.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\python3.6\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: overflow encountered in double_scalars\n",
      "d:\\software\\python3.6\\lib\\site-packages\\ipykernel_launcher.py:77: RuntimeWarning: overflow encountered in double_scalars\n",
      "d:\\software\\python3.6\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "Epoch : 2  Acc: 0.8948675496688742 pre: 0.8948675496688742 rec: 0.9981720377163397 f1: 0.9437010947805864.\n",
      "33\n",
      "Novelty Detection :  Acc: 0.9908940397350994 pre: 0.9908940397350994 rec: 0.9975413907284768 f1: 0.9942066041656322.\n"
     ]
    }
   ],
   "source": [
    "#initialize training set and testing set ->IP_clusters\n",
    "class_num = 20\n",
    "novel_num = 5\n",
    "th = 0.17\n",
    "seed = int(time.time())\n",
    "\n",
    "IP_clusters, novel_classes = data_init(mode='cross',reverse=True, novelty=novel_num,test_rate= 0.2,seed = seed,\n",
    "                                       novel_classes = [])\n",
    "# IP_clusters, novel_classes = data_init(mode='cross',reverse=False, novelty=novel_num,test_rate= 0.2,seed = seed)\n",
    "print(novel_classes)\n",
    "novel_classes = [i for i in range(class_num-novel_num,class_num)]\n",
    "# mark initialized novel clt index\n",
    "novel_idxs = mark_init_novel_index(IP_clusters,novel_classes)\n",
    "\n",
    "\n",
    "#adj_matrix and labels initialization\n",
    "adj_matrix,labels = label_mat_init(IP_clusters)\n",
    "#check out outer clusters\n",
    "outer_clt,max_indx_list = clt_analysis(IP_clusters,adj_matrix)\n",
    "\n",
    "\n",
    "#update labels\n",
    "labels = label_update(IP_clusters,labels)\n",
    "#check out isolate clusters\n",
    "zero_idx = check_zero_label(adj_matrix,labels)\n",
    "zero_clts = [(i, IP_clusters[i]) for i in range(len(IP_clusters)) if i in zero_idx]\n",
    "adj_cand_clts = outer_clt + zero_clts\n",
    "#adjust iso clts to the gt classes\n",
    "print(\"*** Confused nodes should be adjusted to the ground true labels.***\")\n",
    "clts_adj = clt_adj(IP_clusters,adj_cand_clts)\n",
    "\n",
    "#start to propagate\n",
    "print()\n",
    "print(\"Starting to propagate.\")\n",
    "epoch = 1 # A small epoch can achieve satisfactory performance and prevent from overfitting\n",
    "for e in range(epoch):\n",
    "    # nodes aggregate info from neighbours and update the adjoin edges\n",
    "    adj_matrix = aggregation(clts_adj,adj_matrix,labels)\n",
    "    # update the labels\n",
    "#     labels = label_update(clts_adj,labels)\n",
    "    print(len([item for item in labels if item==-1]))\n",
    "    # spread label info to the unlabeled neighbours\n",
    "    labels = spread(clts_adj,adj_matrix,labels,th=th)\n",
    "    # performing measurement in testing set\n",
    "    acc,pre,rec,f1 = acc_per_round(clts_adj,labels,novel_idxs)\n",
    "    f1 = 2*pre*rec/(pre+rec)\n",
    "    print(\"Epoch : {}  Acc: {} pre: {} rec: {} f1: {}.\".format(e,acc,pre,rec,f1))\n",
    "# deal with the isolated nodes, aggregate by time info\n",
    "print(len([item for item in labels if item==-1]))\n",
    "labels = enhc_iso(clts_adj,labels,mode = 'Normal',novel_classes = novel_classes)\n",
    "\n",
    "acc,pre,rec,f1 = acc_per_round(clts_adj,labels,novel_idxs)\n",
    "f1 = 2*pre*rec/(pre+rec)\n",
    "# acc after propagation\n",
    "print(\"Testing Acc: {} pre: {} rec: {} f1: {} after {} epochs.\".format(acc,pre,rec,f1,epoch))\n",
    "print()\n",
    "print(\"**************** Novelty Detection ****************\")\n",
    "check_novel_label(clts_adj,labels,novel_idxs)\n",
    "time.sleep(5)\n",
    "labels = novel_clt_clf(clts_adj,labels,novel_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.9043602800763845 pre: 0.9043602800763845 rec: 0.9244176921811585 f1: 0.9142789947065093 after 1 epochs.\n"
     ]
    }
   ],
   "source": [
    "acc,pre,rec,f1 = acc_per_round_novel(clts_adj,labels)\n",
    "f1 = 2*pre*rec/(pre+rec)\n",
    "# acc after propagation\n",
    "print(\"Testing Acc: {} pre: {} rec: {} f1: {} after {} epochs.\".format(acc,pre,rec,f1,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def novel_AUC(clts_adj,labels,novel_classes):\n",
    "    pre_labels = []\n",
    "    gt_labels = []\n",
    "    for i in range(len(clts_adj)):\n",
    "        if clts_adj[i].label not in novel_classes:\n",
    "            gt_labels.append(0)\n",
    "        else:\n",
    "            gt_labels.append(1)\n",
    "        if labels[i] in novel_classes:\n",
    "            pre_labels.append(1)\n",
    "        else:\n",
    "            pre_labels.append(0)\n",
    "#     print(gt_labels)\n",
    "#     print(pre_labels)\n",
    "    print(roc_auc_score(gt_labels,pre_labels))\n",
    "    print(accuracy_score(gt_labels,pre_labels))\n",
    "    print(f1_score(gt_labels,pre_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement(clts_adj,labels,novel_classes):\n",
    "    #Known Classes:\n",
    "    gt_labels = []\n",
    "    labels_pred = []\n",
    "    for i in range(len(labels)):\n",
    "        if clts_adj[i].label not in novel_classes:\n",
    "            gt_labels.append(clts_adj[i].label)\n",
    "            labels_pred.append(labels[i])\n",
    "    acc = accuracy_score(gt_labels,labels_pred)\n",
    "    pre = recall_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "    rec = precision_score(gt_labels,labels_pred, average=\"weighted\")\n",
    "    f1 = 2*pre*rec/(pre+rec)\n",
    "    print(\"Known Classes : Acc: {} Pre: {} Re: {} F1: {}\".format(acc,pre,rec,f1))\n",
    "    \n",
    "    gt_labels_list = [[] for i in range(len(novel_classes))]\n",
    "    labels_pred_list = [[] for i in range(len(novel_classes))]\n",
    "#     print(labels_pred_list)\n",
    "    for i in range(len(labels)):\n",
    "        if clts_adj[i].label in novel_classes:\n",
    "            gt_labels_list[novel_classes.index(clts_adj[i].label)].append(clts_adj[i].label)\n",
    "            labels_pred_list[novel_classes.index(clts_adj[i].label)].append(labels[i])\n",
    "#     print(labels_pred_list)\n",
    "    for i in range(len(gt_labels_list)):\n",
    "        acc = accuracy_score(gt_labels_list[i],labels_pred_list[i])\n",
    "        pre = recall_score(gt_labels_list[i],labels_pred_list[i],average=\"weighted\")\n",
    "        rec = precision_score(gt_labels_list[i],labels_pred_list[i], average=\"weighted\")\n",
    "        f1 = 2*pre*rec/(pre+rec)\n",
    "        print(\"Novel Classes {} : Acc: {} Pre: {} Re: {} F1: {}\".format(i,acc,pre,rec,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known Classes : Acc: 0.9619377162629758 Pre: 0.9619377162629758 Re: 0.9627287522968487 F1: 0.9623330717223946\n",
      "Novel Classes 0 : Acc: 1.0 Pre: 1.0 Re: 1.0 F1: 1.0\n",
      "Novel Classes 1 : Acc: 0.9464285714285714 Pre: 0.9464285714285714 Re: 1.0 F1: 0.9724770642201834\n",
      "Novel Classes 2 : Acc: 0.9736842105263158 Pre: 0.9736842105263158 Re: 1.0 F1: 0.9866666666666666\n",
      "Novel Classes 3 : Acc: 1.0 Pre: 1.0 Re: 1.0 F1: 1.0\n",
      "Novel Classes 4 : Acc: 0.9285714285714286 Pre: 0.9285714285714286 Re: 1.0 F1: 0.962962962962963\n"
     ]
    }
   ],
   "source": [
    "measurement(clts_adj,labels,novel_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2751 sessions loaded from training set.\n",
      "471 clusters are initialized for training set.\n",
      "106 clusters are initialized for testing set.\n",
      "577 nodes are included in the initialized graph.\n",
      "*** Confused nodes should be adjusted to the ground true labels.***\n",
      "79 no need for adjustment\n",
      "83 no need for adjustment\n",
      "88 no need for adjustment\n",
      "95 no need for adjustment\n",
      "96 no need for adjustment\n",
      "101 no need for adjustment\n",
      "103 no need for adjustment\n",
      "104 no need for adjustment\n",
      "105 no need for adjustment\n",
      "109 no need for adjustment\n",
      "115 no need for adjustment\n",
      "119 no need for adjustment\n",
      "122: 4 -> 18\n",
      "123: 4 -> 18\n",
      "163 no need for adjustment\n",
      "180 no need for adjustment\n",
      "215 no need for adjustment\n",
      "238 no need for adjustment\n",
      "265 no need for adjustment\n",
      "266 no need for adjustment\n",
      "270 no need for adjustment\n",
      "274 no need for adjustment\n",
      "275 no need for adjustment\n",
      "290 no need for adjustment\n",
      "292 no need for adjustment\n",
      "294 no need for adjustment\n",
      "296 no need for adjustment\n",
      "298 no need for adjustment\n",
      "303 no need for adjustment\n",
      "309 no need for adjustment\n",
      "311 no need for adjustment\n",
      "313 no need for adjustment\n",
      "319 no need for adjustment\n",
      "347 no need for adjustment\n",
      "356 no need for adjustment\n",
      "361 no need for adjustment\n",
      "363: 11 -> 10\n",
      "364: 11 -> 7\n",
      "371 no need for adjustment\n",
      "402 no need for adjustment\n",
      "413 no need for adjustment\n",
      "422 no need for adjustment\n",
      "446 no need for adjustment\n",
      "448 no need for adjustment\n",
      "453 no need for adjustment\n",
      "464 no need for adjustment\n",
      "467 no need for adjustment\n",
      "468 no need for adjustment\n",
      "84 no need for adjustment\n",
      "97 no need for adjustment\n",
      "100 no need for adjustment\n",
      "101 no need for adjustment\n",
      "105 no need for adjustment\n",
      "108 no need for adjustment\n",
      "115 no need for adjustment\n",
      "128 no need for adjustment\n",
      "134 no need for adjustment\n",
      "142 no need for adjustment\n",
      "146 no need for adjustment\n",
      "147 no need for adjustment\n",
      "150 no need for adjustment\n",
      "151 no need for adjustment\n",
      "198: 6 -> 19\n",
      "258 no need for adjustment\n",
      "262 no need for adjustment\n",
      "288 no need for adjustment\n",
      "300 no need for adjustment\n",
      "304 no need for adjustment\n",
      "348 no need for adjustment\n",
      "354 no need for adjustment\n",
      "356 no need for adjustment\n",
      "358 no need for adjustment\n",
      "361 no need for adjustment\n",
      "379 no need for adjustment\n",
      "381 no need for adjustment\n",
      "454: 15 -> 18\n",
      "502 no need for adjustment\n",
      "503: 18 -> 4\n",
      "514 no need for adjustment\n",
      "572 no need for adjustment\n",
      "\n",
      "Starting to propagate.\n",
      "106\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 1.0 maxlabel: 0 gt: 0\n",
      "maxvalue: 0.9982328773839564 maxlabel: 1 gt: 1\n",
      "maxvalue: 0.9994397668813843 maxlabel: 1 gt: 1\n",
      "maxvalue: 0.28190805486043774 maxlabel: 1 gt: 1\n",
      "maxvalue: 0.999349184936626 maxlabel: 1 gt: 1\n",
      "maxvalue: 0.12559516684523803 maxlabel: 1 gt: 1\n",
      "maxvalue: 0.06881117559667038 maxlabel: 8 gt: 3\n",
      "maxvalue: 0.5252606396797822 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.9989469635189184 maxlabel: 4 gt: 4\n",
      "maxvalue: 0.9999686767359623 maxlabel: 4 gt: 4\n",
      "maxvalue: 0.11160837813060032 maxlabel: 18 gt: 4\n",
      "maxvalue: 0.9999884765869652 maxlabel: 4 gt: 4\n",
      "maxvalue: 0.9999957607423726 maxlabel: 4 gt: 4\n",
      "maxvalue: 0.9875837978885962 maxlabel: 5 gt: 5\n",
      "maxvalue: 0.2843531404350347 maxlabel: 5 gt: 5\n",
      "maxvalue: 0.9983421629668121 maxlabel: 5 gt: 5\n",
      "maxvalue: 0.9999987787595987 maxlabel: 5 gt: 5\n",
      "maxvalue: 0.9993894760383389 maxlabel: 5 gt: 5\n",
      "maxvalue: 0.9997753140743035 maxlabel: 5 gt: 5\n",
      "maxvalue: 0.9999173309258437 maxlabel: 5 gt: 5\n",
      "maxvalue: 0.9326055832481062 maxlabel: 6 gt: 6\n",
      "maxvalue: 0.9741037638043385 maxlabel: 6 gt: 6\n",
      "maxvalue: 0.9999999996361553 maxlabel: 7 gt: 7\n",
      "maxvalue: 0.9999999999943936 maxlabel: 7 gt: 7\n",
      "maxvalue: 0.2194175669963169 maxlabel: 7 gt: 7\n",
      "maxvalue: 0.9999999999979374 maxlabel: 7 gt: 7\n",
      "maxvalue: 0.999999999999241 maxlabel: 7 gt: 7\n",
      "maxvalue: 0.9999999999997211 maxlabel: 7 gt: 7\n",
      "maxvalue: 0.11614751337007309 maxlabel: 7 gt: 7\n",
      "maxvalue: 0.1811088874592145 maxlabel: 8 gt: 8\n",
      "maxvalue: 0.1984963301132304 maxlabel: 8 gt: 8\n",
      "maxvalue: 0.2823311356544951 maxlabel: 8 gt: 8\n",
      "maxvalue: 0.11598493962405436 maxlabel: 8 gt: 8\n",
      "maxvalue: 0.12516133246902222 maxlabel: 8 gt: 8\n",
      "maxvalue: 0.521427010892511 maxlabel: 8 gt: 8\n",
      "maxvalue: 0.7475825686337747 maxlabel: 8 gt: 8\n",
      "maxvalue: 0.4247433130702767 maxlabel: 9 gt: 9\n",
      "maxvalue: 0.10249656229563575 maxlabel: 19 gt: 9\n",
      "maxvalue: 0.7781154896993331 maxlabel: 10 gt: 10\n",
      "maxvalue: 0.10494585661498095 maxlabel: 10 gt: 10\n",
      "maxvalue: 0.6034900931988486 maxlabel: 10 gt: 10\n",
      "maxvalue: 0.11646476652879646 maxlabel: 12 gt: 10\n",
      "maxvalue: 0.2780255043740913 maxlabel: 10 gt: 10\n",
      "maxvalue: 0.2376310699843746 maxlabel: 11 gt: 11\n",
      "maxvalue: 0.4176358451379196 maxlabel: 11 gt: 11\n",
      "maxvalue: 0.0674645526622412 maxlabel: 8 gt: 11\n",
      "maxvalue: 0.9835114966299795 maxlabel: 12 gt: 12\n",
      "maxvalue: 0.9938703307562213 maxlabel: 12 gt: 12\n",
      "maxvalue: 0.9977362493722315 maxlabel: 12 gt: 12\n",
      "maxvalue: 0.9991660192883037 maxlabel: 12 gt: 12\n",
      "maxvalue: 0.2822626866189333 maxlabel: 13 gt: 13\n",
      "maxvalue: 0.07096997459116833 maxlabel: 14 gt: 14\n",
      "maxvalue: 0.06881731181111185 maxlabel: 14 gt: 14\n",
      "maxvalue: 0.5214082667293559 maxlabel: 14 gt: 14\n",
      "maxvalue: 0.06748111111042437 maxlabel: 14 gt: 14\n",
      "maxvalue: 0.9816295210112581 maxlabel: 14 gt: 14\n",
      "maxvalue: 0.7475683940526112 maxlabel: 14 gt: 14\n",
      "maxvalue: 0.2822626710718521 maxlabel: 15 gt: 15\n",
      "maxvalue: 0.7377224623575445 maxlabel: 16 gt: 16\n",
      "maxvalue: 0.16520622512434738 maxlabel: 16 gt: 16\n",
      "maxvalue: 0.18317956293027576 maxlabel: 17 gt: 17\n",
      "maxvalue: 0.9860318114780343 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.48847410677844527 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.8894609405698234 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.19720461637112907 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.22221384320074403 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.8958630924730094 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.6068177958736504 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.8075168720673763 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.9562800653499881 maxlabel: 18 gt: 18\n",
      "maxvalue: 0.39382715733624263 maxlabel: 19 gt: 19\n",
      "maxvalue: 0.39478231367750183 maxlabel: 19 gt: 19\n",
      "maxvalue: 0.08051099168430138 maxlabel: 8 gt: 19\n",
      "maxvalue: 0.1851710189197189 maxlabel: 8 gt: 19\n",
      "maxvalue: 0.1256999381114772 maxlabel: 12 gt: 19\n",
      "maxvalue: 0.2822767265245451 maxlabel: 19 gt: 19\n",
      "maxvalue: 0.07863497517159482 maxlabel: 8 gt: 19\n",
      "Epoch : 0  Acc: 0.9161676646706587 pre: 0.9161676646706587 rec: 0.9225542724322281 f1: 0.9193498769453998.\n",
      "20\n",
      "Epoch : 1  Acc: 0.9161676646706587 pre: 0.9161676646706587 rec: 0.9225542724322281 f1: 0.9193498769453998.\n",
      "20\n",
      "Epoch : 2  Acc: 0.9161676646706587 pre: 0.9161676646706587 rec: 0.9225542724322281 f1: 0.9193498769453998.\n",
      "20\n",
      "Testing Acc: 0.9273182957393483 pre: 0.9273182957393483 rec: 0.926459846450854 f1: 0.9268888723293578 after 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "#initialize training set and testing set ->IP_clusters\n",
    "IP_clusters = data_init(mode='same',reverse=True, novelty=0,test_rate= 0.2)\n",
    "#adj_matrix and labels initialization\n",
    "adj_matrix,labels = label_mat_init(IP_clusters)\n",
    "#check out outer clusters\n",
    "outer_clt,max_indx_list = clt_analysis(IP_clusters,adj_matrix)\n",
    "\n",
    "#update labels\n",
    "labels = label_update(IP_clusters,labels)\n",
    "#check out isolate clusters\n",
    "zero_idx = check_zero_label(adj_matrix,labels)\n",
    "zero_clts = [(i, IP_clusters[i]) for i in range(len(IP_clusters)) if i in zero_idx]\n",
    "adj_cand_clts = outer_clt + zero_clts\n",
    "#adjust iso clts to the gt classes\n",
    "print(\"*** Confused nodes should be adjusted to the ground true labels.***\")\n",
    "clts_adj = clt_adj(IP_clusters,adj_cand_clts)\n",
    "\n",
    "#start to propagate\n",
    "print()\n",
    "print(\"Starting to propagate.\")\n",
    "epoch = 3 # A small epoch can achieve satisfactory performance and prevent from overfitting\n",
    "for e in range(epoch):\n",
    "    # nodes aggregate info from neighbours and update the adjoin edges\n",
    "    adj_matrix = aggregation(clts_adj,adj_matrix,labels)\n",
    "    # update the labels\n",
    "#     labels = label_update(clts_adj,labels)\n",
    "    print(len([item for item in labels if item==-1]))\n",
    "    # spread label info to the unlabeled neighbours\n",
    "    labels = spread(clts_adj,adj_matrix,labels)\n",
    "    # performing measurement in testing set\n",
    "    acc,pre,rec,f1 = acc_per_round(clts_adj,labels)\n",
    "    f1 = 2*pre*rec/(pre+rec)\n",
    "    print(\"Epoch : {}  Acc: {} pre: {} rec: {} f1: {}.\".format(e,acc,pre,rec,f1))\n",
    "# deal with the isolated nodes, aggregate by time info\n",
    "print(len([item for item in labels if item==-1]))\n",
    "labels = enhc_iso(clts_adj,labels)\n",
    "\n",
    "acc,pre,rec,f1 = acc_per_round(clts_adj,labels)\n",
    "f1 = 2*pre*rec/(pre+rec)\n",
    "# acc after propagation\n",
    "print(\"Testing Acc: {} pre: {} rec: {} f1: {} after {} epochs.\".format(acc,pre,rec,f1,epoch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.9185185185185185 pre: 0.9185185185185185 rec: 0.9542408209074875 f1: 0.9360389738048037 after 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "acc,pre,rec,f1 = acc_per_round(clts_adj,labels)\n",
    "f1 = 2*pre*rec/(pre+rec)\n",
    "# acc after propagation\n",
    "print(\"Testing Acc: {} pre: {} rec: {} f1: {} after {} epochs.\".format(acc,pre,rec,f1,epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
